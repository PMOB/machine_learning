---
layout: math
title: はじめてのパターン認識 3章
---
単純ベイズ分類器(naive bayes)
========
本章では観測データ x について、
それが所属されるクラスの間に確率分布が仮定される
**分類問題**を、
**単純ベイズ分類器(naive bayes)** を中心に説明する。


導入; 前提知識
--------
単純ベイズ分類器を扱うために前提知識について触れておく。


### 条件付き確率
事象Aが起きた前提で事象Bが起きる確率を条件付き確率といい、
以下の式で定義される。

$$
P(B | A) = \frac{P(A \cap B)}{P(A)}
$$

特に、

$$
P(B | A) = \frac{P(A \cap B)}{P(A)} \\
P(A)P(B | A) = P(A \cap B)
$$


### ベイズの定理
事象 $A, B$ について、
$P(A | B), P(A), P(B)$がわかっているとき、
$P(B | A)$を以下の式で求めることができる。

$$
P(B | A) = \frac{P(B)P(A | B)}{P(A)}
$$

これをベイズの定理とよぶ。
導出は以下。

$$
P(B | A) = \frac{P(B \cap A)}{P(A)} \\
ここで、 \\
P(B \cap A) = P(B)P(A | B) \\
から、 \\
P(B | A) = \frac{P(B)P(A | B)}{P(A)}
$$


### 尤度
データ $\mathbf{x}$ がクラス $C$ に属する尤もらしさとは、
クラス $C$ であるもののうちデータ $\mathbf{x}$ となるもの
すなわち $p(\mathbf{x} | C)$であり、これを尤度という。


### 条件付き確率と尤度の違い
条件付き確率(確率)が **真の確率変数** を扱うのに対し、
尤度(統計)は **データから算出した値** である。


### 統計学でのベイズの定理
求める$P(B|A)$を、$A$が起きた後の **事後確率** と呼ぶ。
$P(B)$は$A$を考えない時の生起確率であり、事後確率に対して **事前確率** と呼ぶ。
$P(A)$は独立な$B_i$に対して$P(A)=\sum P(B_i \cap A)$であることから、$P(B|A)$についての **周辺確率** と呼ぶ。

これらを用いてベイズの定理を考えると、以下のように解釈できる。

$$
P(B | A) = \frac{P(B)P(A | B)}{P(A)} \\
P(B | A) = \frac{P(A | B)}{P(A)} \times P(B) \\
事後確率 = \frac{尤度}{周辺確率} \times 事前確率
$$


単純ベイズ分類器
--------
ある特徴ベクトルがどのクラスに分類されるかを決定する写像を、
**分類器** とよぶ。
ここでは先ほどのベイズの定理を用いた
**単純ベイズ分類器(naive bayes)** を紹介する。

「特徴ベクトル $\mathbf{x_j}$ がクラス $C_i (i=1,2,\cdots,n)$に分類される」という学習によって
$p(x|C_i), p(\mathbf{x}), p(C_i)$を知っている時、
ある $\mathbf{x}$ がクラス $C_i$ に属する尤度 $p(C_i|\mathbf{x})$ はベイズの定理から、

$$
P(C_i | \mathbf{x}) = \frac{p(\mathbf{x} | C_i)}{p(\mathbf{x})} \times P(C_i)
$$

で導くことができる。
ここで、$C_i$と$C_j$を比較したとき、$\mathbf{x}$ は尤度が高い方のクラスに属する方が尤もらしいといえる。
すなわち、

$$
p(C_i | \mathbf{x}) = \frac{P(C_i)p(\mathbf{x} | C_i)}{p(\mathbf{x})}
\left\{\begin{matrix}
> \\
<
\end{matrix}\right\}
\frac{P(C_j)p(\mathbf{x} | C_j)}{p(\mathbf{x})}= p(C_j | \mathbf{x})
\left\{\begin{matrix}
\Rightarrow C_i \\
\Rightarrow C_j
\end{matrix}\right\}
$$

ここで $p(\mathbf{x})$ をはらって、

$$
P(C_i)p(\mathbf{x} | C_i)
\left\{\begin{matrix}
> \\
<
\end{matrix}\right\}
P(C_j)p(\mathbf{x} | C_j)
\left\{\begin{matrix}
\Rightarrow C_i \\
\Rightarrow C_j
\end{matrix}\right\}
$$

を得る。全クラス$C_i (i=1,2,\cdots,n)$に対して行えばよいので、
単純ベイズ分類器は以下の式で分類を行う。

$$
\max_{i} p(x | C_i)P(C_i)
$$

また、クラス$C_i$と$C_j$についての識別境界は

$$
P(C_i | \mathbf{x}) = \frac{p(\mathbf{x} | C_i)}{p(\mathbf{x})} \times P(C_i) =
\frac{p(\mathbf{x} | C_j)}{p(\mathbf{x})} \times P(C_j) = P(C_j | \mathbf{x})
$$

にある。


誤り率
========
あるクラス$C_i$と$C_j$に同様の特徴が現れた時、
分類器は分類を誤る場合がある。
誤りが発生する確率を **誤り率$\varepsilon$** とする。
単純ベイズ分類器はクラス$C_i$の事後確率がクラス$C_j$の事後確率より大きい時に
クラス$C_i$に分類するため、誤りは

$$
\varepsilon(\mathbf{x}) = \min [P(C_i|\mathbf{x}), P(C_j|\mathbf{x})]
$$

となる。

期待値が最小になる話はちょっとまってください。


リジェクト
========
曖昧な部分について誤りを避けるために閾値を設けてリジェクトを行う。
閾値$t$を用いて、

$$
\begin{matrix}
C_i \Leftarrow P(C_i|\mathbf{x}) = \max_{j} P(C_j|\mathbf{x}) > 1-t \\
リジェクト \Leftarrow {}^{\nexists}\left(\max_{j} P(C_j|\mathbf{x})\right) > 1-t
\end{matrix}
$$



損失
========
健康な人(1)と病気の人(2)を判断する時の誤認識には、
健康な人を病気であると認識する$\varepsilon_{12}$と
病気の人を健康であると認識する$\varepsilon_{21}$が存在する。
ここで単純ベイズ分類器は誤り最小となる境界を設けるが、
明らかに *病気の人を健康であると誤診* する方が危険であるため、
どうにかしてこの誤診を減らしたい。
ここでは **損失** の導入で
$\varepsilon_{21}$の期待値を減らすことができることを示す。

### 最小損失基準によるベイズの識別規則
真のクラスが$C_i$であるデータを$C_j$と誤ったときの危険性を損失$Lij$で表す。
K個のクラスがあるとき$Lij$を要素とする$K \times K$の行列が作られる。
これを **損失行列** という。

ここから、
データ$\mathbf{x}$をクラス$C_i$と判断した時に発生する損失は、

$$
r(C_i | \mathbf{x}) = \sum_{k=1}^K L_{ik}P(C_k | \mathbf{x})
$$

となる。
ここで識別規則を、*「損失が最も小さいクラスに識別すること」* とする。
すなわち

$$
識別クラス = \min_{i} r(C_i | \mathbf{x})
$$


ROC曲線
========

ROC曲線の性質
--------

ROC曲線による性能評価
--------

単純ベイズ分類の例
========

|                 | サンプル数 | 喫煙する人(S=1) | 飲酒する人(T=1) |
|-----------------|------------|-----------------|-----------------|
| 健康(G=1)       | 800        | 320             | 640             |
| 健康でない(G=0) | 200        | 160             | 40              |
